{
  "total_chunks": 27,
  "chunks": [
    {
      "source": "compliance_doc.pdf",
      "chunk_id": 0,
      "text": "Proceedings of the 2010 Winter Simulation Conference B. Johansson, S. Jain, J. Montoya-Torres, J. Hugan, and E. Y\u00fccesan, eds. VERIFICATION AND VALIDATION OF SIMULATION MODELS Robert G. Sargent Department of Electrical Engineering and Computer Science L. C. Smith College of Engineering and Computer Science Syracuse University Syracuse, NY 13244, USA ABSTRACT In this paper we discuss verification and validation of simulation models. Four different approaches to de- ciding model validity are described; two different paradigms that relate verification and validation to the model development process are presented; various validation techniques are defined; conceptual model validity, model verification, operational validity, and data validity are discussed; a way to document re- sults is given; a recommended procedure for model validation is presented; and model accreditation is briefly discussed. 1 INTRODUCTION Simulation models are increasingly being used to solve problems and to aid in decision-making. The de- velopers and users of these models, the decision makers using information obtained from the results of these models, and the individuals affected by decisions based on such models are all rightly concerned with whether a model and its results are \u201ccorrect\u201d. This concern is addressed through model verification and validation. Model verification is often defined as \u201censuring that the computer program of the compu- terized model and its implementation are correct\u201d and is the definition adopted here. Model validation is usually defined to mean \u201csubstantiation that a computerized model within its domain of applicability pos- sesses a satisfactory range of accuracy consistent with the intended application of the model\u201d (Schlesinger et al. 1979) and is the definition used here. A model sometimes becomes accredited through model accre- ditation. Model accreditation determines if a model satisfies specified model accreditation criteria accord- ing to a specified process. A related topic is model credibility. Model credibility is concerned with devel- oping in (potential) users the confidence they require in order to use a model and in the information derived from that model. A model should be developed for a specific purpose (or application) and its validity determined with respect to that purpose. If the purpose of a model is to answer a variety of questions, the validity of the model needs to be determined with respect to each question. Numerous sets of experimental conditions are usually required to define the domain of a model\u2019s intended applicability. A model may be valid for one set of experimental conditions and invalid in another. A model is considered valid for a set of expe- rimental conditions if the model\u2019s accuracy is within its acceptable range, which is the amount of accura- cy required for the model\u2019s intended purpose. This usually requires that the model\u2019s output variables of interest (i.e., the model variables used in answering the questions that the model is being developed to an- swer) be identified and that their required amount of accuracy be specified. The amount of accuracy re- quired should be specified prior to starting the development of the model or very early in the model de- velopment"
    },
    {
      "source": "compliance_doc.pdf",
      "chunk_id": 1,
      "text": "process. If the variables of interest are random variables, then properties and functions of the random variables such as means and variances are usually what is of primary interest and are what is used in determining model validity. Several versions of a model are usually developed prior to obtaining a sa- tisfactory valid model. The substantiation that a model is valid, i.e., performing model verification and va- 166 978-1-4244-9864-2/10/$26.00 \u00a92010 IEEESargent lidation, is generally considered to be a process and is usually part of the (total) model development process. It is often too costly and time consuming to determine that a model is absolutely valid over the com- plete domain of its intended applicability. Instead, tests and evaluations are conducted until sufficient con-fidence is obtained that a model can be considered valid for its intended application (Sargent 1982, 1984a). If a test determines that a model does not have sufficient accuracy for any one of the sets of expe- rimental conditions, then the model is invalid. However, determining that a model has sufficient accuracy for numerous experimental conditions does not guarantee that a model is valid everywhere in its applica- ble domain. Figure 1 shows the relationships between model confidence and (a) cost (a similar relation- ship holds for the amount of time) of performing model validation and (b) the value of the model to a us- er. The cost of model validation is usually quite significant, especially when extremely high model confidence is required. Figure 1: Model Confidence The remainder of this paper is organized as follows: Section 2 presents the basic approaches used in deciding model validity; Section 3 describes two different paradigms used in verification and validation; Section 4 defines validation techniques; Sections 5, 6, 7, and 8 discuss data validity, conceptual model va- lidity, computerized model verification, and operational validity, respectively; Section 9 describes a way of documenting results; Section 10 gives a recommended validation procedure; Section 11 contains a brief description of accreditation; and Section 12 presents the summary. (Note that this tutorial paper is similar to Sargent (2009).) 2 BASIC APPROACHES There are four basic decision-making approaches for deciding whether a simulation model is valid. Each of the approaches requires the model development team to conduct verification and validation as part of the model development process, which is discussed in Section 3. One approach, and a frequently used one, is for the model development team itself to make the decision as to whether a simulation model is va- lid. A subjective decision is made based on the results of the various tests and evaluations conducted as part of the model development process. However, it is usually better to use one of the next two approach- es, depending on which situation applies. If the size of the simulation team developing the model is small, a better approach than the one above is to have the user(s) of the model heavily involved with the model development team in deciding the va- lidity of the simulation model."
    },
    {
      "source": "compliance_doc.pdf",
      "chunk_id": 2,
      "text": "In this approach the focus of determining the validity of the simulation model moves from the model developers to the model users. Also, this approach aids in model credibility. Another approach, usually called \u201cindependent verification and validation\u201d (IV&V), uses a third (in- dependent) party to decide whether the simulation model is valid. The third party is independent of both the simulation development team(s) and the model sponsor/user(s). The IV&V approach should be used when developing large-scale simulation models, whose developments usually involve several teams. (This approach is also useful in model credibility, especially when the problem the simulation model is associated with has a high cost.) The third party needs to have a thorough understanding of the intended 167Sargent purpose(s) of the simulation model in order to conduct IV&V. There are two common ways that the third party conducts IV&V: (a) IV&V is conducted concurrently with the development of the simulation model and (b) IV&V is conducted after the simulation model has been developed. In the concurrent way of conducting IV&V, the model development team(s) receives inputs from the IV&V team regarding verification and validation as the model is being developed. When conducting IV&V this way, the development of a simulation model should not progress to the next stage of develop- ment until the model has satisfied the verification and validation requirements in its current stage. It is the author\u2019s opinion that this is the better of the two ways to conduct IV&V. When IV&V is conducted after the simulation model has been completely developed, the evaluation performed can range from simply evaluating the verification and validation conducted by the model de-velopment team to performing a complete verification and validation effort. Wood (1986) describes expe- riences over this range of evaluation by a third party on energy models. One conclusion that Wood makes is that performing a complete IV&V effort after the simulation model has been completely developed is both extremely costly and time consuming, especially for what is obtained. This author\u2019s view is that if IV&V is going to be conducted on a completed simulation model then it is usually best to only evaluate the verification and validation that has already been performed. The last approach for determining whether a model is valid is to use a scoring model. (See Balci (1989), Gass (1993), and Gass and Joel (1987) for examples of scoring models.) Scores (or weights) are determined subjectively when conducting various aspects of the validation process and then combined to determine category scores and an overall score for the simulation model. A simulation model is consi- dered valid if its overall and category scores are greater than some passing score(s). This approach is sel- dom used in practice. This author does not believe in the use of scoring models for determining validity because (1) a model may receive a passing score and yet have a defect that needs to be corrected, (2) the subjectiveness of this approach tends to be hidden resulting in this approach appearing to"
    },
    {
      "source": "compliance_doc.pdf",
      "chunk_id": 3,
      "text": "be objective, (3) the passing scores must be decided in some (usually) subjective way, (4) the score(s) may cause over confidence in a model, and (5) the scores can be used to argue that one model is better than another. 3 PARADIGMS In this section we present and discuss paradigms that relate verification and validation to the model de- velopment process. There are two common ways to view this relationship. One way uses a simple view and the other uses a complex view. Banks, Gerstein, and Searles (1988) reviewed work using both of these ways and concluded that the simple way more clearly illuminates model verification and validation. We present one paradigm for each way developed by this author. The paradigm of the simple way is pre- sented first and is this author\u2019s preferred paradigm. Consider the simplified version of the model development process in Figure 2 (Sargent 1981). The problem entity is the system (real or proposed), idea, situation, policy, or phenomena to be modeled; the conceptual model is the mathematical/logical/verbal representation (mimic) of the problem entity devel- oped for a particular study; and the computerized model is the conceptual model implemented on a com- puter. The conceptual model is developed through an analysis and modeling phase , the computerized model is developed through a computer programming and implementation phase , and inferences about the problem entity are obtained by conducting computer experiments on the computerized model in the experimentation phase . We now relate model validation and verification to this simplified version of the modeling process. (See Figure 2.) Conceptual model validation is defined as determining that the theories and assumptions underlying the conceptual model are correct and that the model representation of the problem entity is \u201creasonable\u201d for the intended purpose of the model. Computerized model verification is defined as assur- ing that the computer programming and implementation of the conceptual model is correct. Operational validation is defined as determining that the model\u2019s output behavior has sufficient accuracy for the mod- 168Sargent Figure 2: Simplified Version of the Modeling Process el\u2019s intended purpose over the domain of the model\u2019s intended applicability. Data validity is defined as ensuring that the data necessary for model building, model evaluation and testing, and conducting the model experiments to solve the problem are adequate and correct. An iterative process is used to develop a valid simulation model (Sargent 1984a). A conceptual model is developed followed by conceptual model validation. This process is repeated until the conceptual mod- el is satisfactory. Next the computerized model is developed from the conceptual model followed by computerized model verification. This process is repeated until the computerized model is satisfactory. Next operational validity is conducted on the computerized model. Model changes required by conducting operational validity can be in either the conceptual model or in the computerized model. Verification and validation must be performed again when any model change is made. Usually several models are devel- oped prior to obtaining a valid simulation model. A detailed way"
    },
    {
      "source": "compliance_doc.pdf",
      "chunk_id": 4,
      "text": "of relating verification and validation to developing simulation models and system theories is shown in Figure 3. This paradigm shows the processes of developing system theories and si- mulation models and relates verification and validation to both of these processes. This paradigm (Sargent 2001b) shows a Real World and a Simulation World. We first discuss the Real World. There exist some system or problem entity in the real world of which an understanding of is desired. System theories describe the characteristics and the causal relationships of the system (or problem entity) and possibly its behavior (including data). System data and results are obtained by conducting ex- periments ( experimenting) on the system. System theories are developed by abstracting what has been observed from the system and by hypothesizing from the system data and results. If a simulation model exists of this system, then hypothesizing of system theories can also be done from simulation data and re- sults. System theories are validated by performing theory validation. Theory validation involves the com- parison of system theories against system data and results over the domain the theory is applicable for to determine if there is agreement. This process requires numerous experiments to be conducted on the real system. We now discuss the Simulation World, which shows a slightly more complicated model development process than the previous paradigm (Figure 2). A simulation model should only be developed for a set of well-defined objectives. The conceptual model is the mathematical/logical/verbal representation (mimic) of the system developed for the objectives of a particular study. The simulation model specification is a written detailed description of the software design and specification for programming and implementing the conceptual model on a particular computer system. The simulation model is the conceptual model Problem Entity (System) Conceptual Model Data Validity Computerized Model Verification Computer Programming and Implementation Conceptual Model Validation Analysis and Modeling Experimentation Operational Validation Computerized Model 169Sargent Figure 3: Real World and Simulation World Relationships with Verification and Validation running on a computer system such that experiments can be conducted on the simulation model. The si- mulation model data and results are the data and results from experiments conducted ( experimenting ) on the simulation model. The conceptual model is developed by modeling the system for the objectives of the simulation study using the understanding of the system contained in the system theories. The simula- tion model is obtained by implementing the model on the specified computer system, which includes pro- gramming the conceptual model whose specifications are contained in the simulation model specification. Inferences about the system are made from data obtained by conducting computer experiments ( experi- menting) on the simulation model. Conceptual model validation is defined as determining that the theories and assumptions underlying the conceptual model are consistent with those in the system theories and that the model representation of the system is \u201creasonable\u201d for the intended purpose of the simulation model. Specification verification is defined as assuring that the software design and the specification"
    },
    {
      "source": "compliance_doc.pdf",
      "chunk_id": 5,
      "text": "for programming and implementing the conceptual model on the specified computer system is satisfactory. Implementation verification is defined as assuring that the simulation model has been implemented according to the simulation model specifica- tion. Operational validation is defined as determining that the model\u2019s output behavior has sufficient ac- curacy for the model\u2019s intended purpose over the domain of the model\u2019s intended applicability. ADDITIONAL EXPERIMENTS (TESTS) NEEDED SYSTEM (PROBLEM ENTITY) SYSTEM DATA/RESULTS REA L WORLD EXPERIMENTING ABSTRACTING HYPOTHESIZING SYSTEM EXPERIMENT OBJECTIVES Theory Validation SIMULATION EXPERIMENT OBJECTIVES SIMULATION MODEL DATA/RESULTS SIMULATION MODEL SPECIFICATION SIMULATION MODEL SIMULATION WORLD SPECIFYING EXPERIMENTING IMPLEMENTING HYPOTHESIZING MODELING SYSTEM THEORIES CONCEPTUAL MODEL Operational (Results) Validation Conceptual Model Validation Specification Verification Implementation Verification 170Sargent This paradigm shows processes for developing both valid system theories and valid simulation mod- els. Both are accomplished through iterative processes. To develop valid system theories, which are usually for a specific purpose, the system is first observed and then abstraction is performed from what has been observed to develop proposed system theories. These theories are tested for correctness by con- ducting experiments on the system to obtain data and results to compare against the proposed system theories. New proposed system theories may be hypothesized from the data and comparisons made, and also possibly from abstraction performed on additional system observations. These new proposed theories will require new experiments to be conducted on the system to obtain data to evaluate the correctness of these proposed system theories. This process repeats itself until a satisfactory set of validated system theories has been obtained. To develop a valid simulation model, a process similar to the one for the pre- vious paradigm is performed with the only difference being more detail is given in this paradigm. Several versions of a model are usually developed prior to obtaining a satisfactory valid simulation model. Model verification and validation must be performed in each model iteration. 4 VALIDATION TECHNIQUES This section describes validation techniques and tests commonly used in model verification and vali- dation. Most of the techniques described here are found in the literature, although some may be described slightly differently. They can be used either subjectively or objectively. By \u201cobjectively,\u201d we mean using some type of mathematical procedure or statistical test, e.g., hypothesis tests or confidence intervals. A combination of techniques is generally used. These techniques are used for verifying and validating the submodels and the overall model. Animation: The model\u2019s operational behavior is displayed graphically as the model moves through time. E.g., the movements of parts through a factory during a simulation run are shown graphically. Comparison to Other Models: Various results (e.g., outputs) of the simulation model being validated are compared to results of other (valid) models. For example, (1) simple cases of a simulation model are compared to known results of analytic models, and (2) the simulation model is compared to other simula- tion models that have been validated. Degenerate Tests: The degeneracy of the model\u2019s behavior is tested by appropriate selection of val- ues of"
    },
    {
      "source": "compliance_doc.pdf",
      "chunk_id": 6,
      "text": "the input and internal parameters. For example, does the average number in the queue of a single server continue to increase over time when the arrival rate is larger than the service rate? Event Validity: The \u201cevents\u201d of occurrences of the simulation model are compared to those of the real system to determine if they are similar. For example, compare the number of fires in a fire department si-mulation to the actual number of fires. Extreme Condition Tests: The model structure and outputs should be plausible for any extreme and unlikely combination of levels of factors in the system. For example, if in-process inventories are zero, production output should usually be zero. Face Validity: Individuals knowledgeable about the system are asked whether the model and/or its behavior are reasonable. For example, is the logic in the conceptual model correct and are the model\u2019s in- put-output relationships reasonable. Historical Data Validation: If historical data exist (e.g., data collected on a system specifically for building and testing a model), part of the data is used to build the model and the remaining data are used to determine (test) whether the model behaves as the system does. (This testing is conducted by driving the simulation model with either samples from distributions or traces (Balci and Sargent 1982a, 1982b, 1984b).) Historical Methods: The three historical methods of validation are rationalism , empiricism , and posi- tive economics . Rationalism assumes that everyone knows whether the clearly stated underlying assump- tions of a model are true. Logic deductions are used from these assumptions to develop the correct (valid) model. Empiricism requires every assumption and outcome to be empirically validated. Positive econom- ics requires only that the model be able to predict the future and is not concerned with a model\u2019s assump- tions or structure (causal relationships or mechanisms). 171Sargent Internal Validity: Several replications (runs) of a stochastic model are made to determine the amount of (internal) stochastic variability in the model. A large amount of variability (lack of consistency) may cause the model\u2019s results to be questionable and if typical of the problem entity, may question the appro- priateness of the policy or system being investigated. Multistage Validation: Naylor and Finger (1967) proposed combining the three historical methods of rationalism, empiricism, and positive economics into a multistage process of validation. This validation method consists of (1) developing the model\u2019s assumptions on theory, observations, and general know- ledge, (2) validating the model\u2019s assumptions where possible by empirically testing them, and (3) com- paring (testing) the input-output relationships of the model to the real system. Operational Graphics: Values of various performance measures, e.g., the number in queue and per- centage of servers busy, are shown graphically as the model runs through time; i.e., the dynamical beha-viors of performance indicators are visually displayed as the simulation model runs through time to en- sure they behave correctly. Parameter Variability - Sensitivity Analysis: This technique consists of changing the values of the in- put and internal parameters of a model"
    },
    {
      "source": "compliance_doc.pdf",
      "chunk_id": 7,
      "text": "to determine the effect upon the model\u2019s behavior or output. The same relationships should occur in the model as in the real system. This technique can be used qualitative- ly\u2014directions only of outputs\u2014and quantitatively\u2014both directions and (precise) magnitudes of outputs. Those parameters that are sensitive, i.e., cause significant changes in the model\u2019s behavior or output, should be made sufficiently accurate prior to using the model. (This may require iterations in model de- velopment.) Predictive Validation: The model is used to predict (forecast) the system\u2019s behavior, and then com- parisons are made between the system\u2019s behavior and the model\u2019s forecast to determine if they are the same. The system data may come from an operational system or be obtained by conducting experiments on the system, e.g., field tests. Traces: The behaviors of different types of specific entities in the model are traced (followed) through the model to determine if the model\u2019s logic is correct and if the necessary accuracy is obtained. Turing Tests: Individuals who are knowledgeable about the operations of the system being modeled are asked if they can discriminate between system and model outputs. (Schruben (1980) contains statistic-al tests for Turing tests.) 5 DATA VALIDITY We discuss data validity, even though it is often not considered to be part of model validation, because it is usually difficult, time consuming, and costly to obtain appropriate, accurate, and sufficient data, and da- ta problems are often the reason that attempts to valid a model fail. Data are needed for three purposes: for building the conceptual model, for validating the model, and for performing experiments with the va- lidated model. In model validation we are usually concerned only with data for the first two purposes. To build a conceptual model we must have sufficient data on the problem entity to develop theories that can be used to build the model, to develop mathematical and logical relationships for use in the mod- el that will allow the model to adequately represent the problem entity for its intended purpose, and to test the model\u2019s underlying assumptions. In addition, behavioral data are needed on the problem entity to be used in the operational validity step of comparing the problem entity\u2019s behavior with the model\u2019s beha- vior. (Usually, this data are system input/output data.) If behavior data are not available, high model con- fidence usually cannot be obtained because sufficient operational validity cannot be achieved. The concerns with data are that appropriate, accurate, and sufficient data are available, and all data transformations, such as data disaggregation, are made correctly. Unfortunately, there is not much that can be done to ensure that the data are correct. One should develop good procedures for (1) collecting and maintaining data, (2) testing the collected data using techniques such as internal consistency checks, and (3) screening the data for outliers and determining if the outliers are correct. If the amount of data is large, a database of the data should be developed and maintained. 172Sargent 6 CONCEPTUAL MODEL VALIDATION Conceptual model"
    },
    {
      "source": "compliance_doc.pdf",
      "chunk_id": 8,
      "text": "validity is determining that (1) the theories and assumptions underlying the conceptual model are correct and (2) the model\u2019s representation of the problem entity and the model\u2019s structure, log- ic, and mathematical and causal relationships are \u201creasonable\u201d for the intended purpose of the model. The theories and assumptions underlying the model should be tested using mathematical analysis and statistic- al methods on problem entity data. Examples of theories and assumptions are linearity, independence of data, and arrivals are Poisson. Examples of applicable statistical methods are fitting distributions to data, estimating parameter values from the data, and plotting data to determine if the data are stationary. In ad- dition, all theories used should be reviewed to ensure they were applied correctly. For example, if a Mar- kov chain is used, does the system have the Markov property, and are the states and transition probabili- ties correct? Each submodel and the overall model must be evaluated to determine if they are reasonable and cor- rect for the intended purpose of the model. This should include determining if the appropriate detail and aggregate relationships have been used for the model\u2019s intended purpose, and also if appropriate structure, logic, and mathematical and causal relationships have been used. The primary validation techniques used for these evaluations are face validation and traces. Face validation has experts on the problem entity eva- luate the conceptual model to determine if it is correct and reasonable for its purpose. This usually re- quires examining the flowchart or graphical model (Sargent 1986), or the set of model equations. The use of traces is the tracking of entities through each submodel and the overall model to determine if the logic is correct and if the necessary accuracy is maintained. If errors are found in the conceptual model, it must be revised and conceptual model validation performed again. 7 COMPUTERIZED MODEL VERIFICATION Computerized model verification ensures that the computer programming and implementation of the con-ceptual model are correct. The major factor affecting verification is whether a simulation language or a higher level programming language such as FORTRAN, C, or C++ is used. The use of a special-purpose simulation language generally will result in having fewer errors than if a general-purpose simulation lan- guage is used, and using a general-purpose simulation language will generally result in having fewer er- rors than if a general purpose higher level programming language is used. (The use of a simulation lan- guage also usually reduces both the programming time required and the amount of flexibility.) When a simulation language is used, verification is primarily concerned with ensuring that an error free simulation language has been used, that the simulation language has been properly implemented on the computer, that a tested (for correctness) pseudo random number generator has been properly imple- mented, and that the model has been programmed correctly in the simulation language. The primary tech- niques used to determine that the model has been programmed correctly are structured walkthroughs and traces. If a higher level programming"
    },
    {
      "source": "compliance_doc.pdf",
      "chunk_id": 9,
      "text": "language has been used, then the computer program should have been designed, developed, and implemented using techniques found in software engineering. (These include such techniques as object-oriented design, structured programming, and program modularity.) In this case verification is primarily concerned with determining that the simulation functions (e.g., the time-flow me- chanism, pseudo random number generator, and random variate generators) and the computerized (simu- lation) model have been programmed and implemented correctly. There are two basic approaches for testing simulation software: static testing and dynamic testing (Fairley 1976). In static testing the computer program is analyzed to determine if it is correct by using such techniques as structured walkthroughs, correctness proofs, and examining the structure properties of the program. In dynamic testing the computer program is executed under different conditions and the val- ues obtained (including those generated during the execution) are used to determine if the computer pro- gram and its implementations are correct. The techniques commonly used in dynamic testing are traces, investigations of input-output relations using different validation techniques, internal consistency checks, 173Sargent and reprogramming critical components to determine if the same results are obtained. If there are a large number of variables, one might aggregate the numerical values of some of the variables to reduce the number of tests needed or use certain types of design of experiments (Kleijnen 1987). It is necessary to be aware while checking the correctness of the computer program and its implemen- tation that errors found may be caused by the data, the conceptual model, the computer program, or the computer implementation. (See Whitner and Balci (1989) for a detailed discussion on model verification.) 8 OPERATIONAL VALIDITY Operational validation is determining whether the simulation model\u2019s output behavior has the accuracy required for the model\u2019s intended purpose over the domain of the model\u2019s intended applicability. This is where much of the validation testing and evaluation take place. Since the simulation model is used in op- erational validation, any deficiencies found may be caused by what was developed in any of the steps that are involved in developing the simulation model including developing the system\u2019s theories or having invalid data. All of the validation techniques discussed in Section 4 are applicable to operational validity. Which techniques and whether to use them objectively or subjectively must be decided by the model develop- ment team and the other interested parties. The major attribute affecting operational validity is whether the problem entity (or system) is observable, where observable means it is possible to collect data on the operational behavior of the problem entity. Table 1 gives a classification of the validation techniques used in operational validity based on the decision approach and system observable. \u201cComparison\u201d means com-paring the simulation model output behavior to either the system output behavior or another model output behavior using graphical displays or statistical tests and procedures. \u201cExplore model behavior\u201d means to examine the output behavior of the simulation model using appropriate validation techniques, including parameter variability-sensitivity analysis. Various sets of experimental conditions"
    }
  ]
}